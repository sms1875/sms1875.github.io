---
layout: post
title: "데이터 품질 문제"
date: 2025-01-03 11:27:00+0900
categories: [Study, AI]
tags: [ML, DL, Data]
math: true
mermaid : true
---
## 데이터 품질 문제 유형

### **결측치(Missing Value)**

- Null 값
- 수집된 데이터셋 중 **관측되지 않거나 누락된 데이터**
- 발생 시 잘못된 분석 결과를 낳거나 분석 시 에러가 발생할 수 있음
- 원인
    - 센서 오작동, 네트워크 문제 등
    - 설문조사에서 응답자가 특정 질문에 답하지 않는 경우
    - 데이터를 입력하는 과정에서 값이 누락되거나 잘못 기록
    - 데이터베이스 손상, 저장 프로세스의 오류
- 예시
    
    
    | **ID** | **날짜(Date)** | **온도(Temperature)** | **습도(Humidity)** | **풍속(Wind Speed)** | **비고(Notes)** |
    | --- | --- | --- | --- | --- | --- |
    | 1 | 2024-12-01 | 15.5 | 45 | 3.2 | 정상 |
    | 2 | 2024-12-02 | - | 50 | 4.1 | 온도 센서 오류 |
    | 3 | 2024-12-03 | 16.0 | - | 2.9 | 습도 데이터 누락 |
    | 4 | 2024-12-04 | 17.2 | 48 | - | 풍속 데이터 미기록 |
    | 5 | 2024-12-05 | - | - | - | 기록 과정에서 문제 발생 |
    | 6 | 2024-12-06 | 14.3 | 42 | 3.0 | 정상 |

#### **종류**

1. **MCAR(Missing Completely At Random, 완전 무작위 결측)**
    - 결측치가 완전히 랜덤하게 발생
    - 결측 여부가 데이터 내 모든 변수와 상관없음
    - 데이터가 충분히 크다면 분석 결과에 영향을 미치지 않음
    - 해결 방법
        - 결측치 제거
        - 평균, 중앙값 등으로 대체
    - 예시
        - 센서 오류 및 네트워크 장애로 일부 데이터 유실
2. **MAR(Missing At Random, 무작위 결측)**
    - 결측 여부가 관측된 다른 변수에 의해 설명가능
    - 결측치 자체는 랜덤이 아니지만, 관측된 데이터로 예측가능
    - 해결 방법
        - 관측된 변수 기반 예측 모델 활용(회귀, KNN)
        - 분포 기반 대체(조건부 평균)
    - 예시
        - 환자의 건강 기록 중 약물 사용 정보 누락, 다른 건강 지표로 추측 가능
3. **MNAR(Missing Not At Random, 비 무작위 결측)**
    - 결측 여부가 데이터 내 관측되지 않은 변수와 관련
    - 결측 자체가 특정 이유로 발생하며, 그 원인을 찾기 쉽지 않음
    - 해결 방법
        - 도메인 지식을 통한 결측 원인 파악 후 대체
        - 결측치 자체를 새로운 범주로 설정(미응답)
    - 예시
        - 응답자들이 고의로 사실과 다른 응답을 하는 경우(소득범위, 우울증)

### **이상치(Outlier)**

- 관측된 데이터의 **범위에서 많이 벗어난 아주 작은 값이나 큰 값**
- 데이터 분석 혹은 모델링에서 의사결정에 큰 영향을 미침
- 원인
    - 잘못된 데이터 입력 또는 장비 오류
    - 데이터 분포 내에서 극단적인 값 발생
    - 데이터 수집에서 샘플링 과정에서 발생한 오류

![https://yozm.wishket.com/media/news/1919/image001.png](https://yozm.wishket.com/media/news/1919/image001.png)

#### **탐지 방법**

1. **Z-Score**
    - 데이터의 분포가 정규 분포를 이룰 때, 데이터의 표준 편차를 이용해 이상치를 탐지하는 방법
    - 해당 데이터가 평균으로부터 얼마나 표준편차 만큼 벗어나 있는지 의미
    - 주로 3 이상, -3이하 값을 이상치로 판별함
        
        ![image.png](assets/img/posts/study/AI/데이터 품질 문제/image.png)
        
2. **IQR(Interquartile Range) with Box plots**
    - 데이터의 분포가 정규를 이루지 않거나 한쪽으로 치우친 경우 IQR 값을 사용해 이상치를 탐지하는 방법
    - 박스 플롯을 통해 최소값, 최대값, 중앙값, Q1, Q3 값을 알 수 있음
    - Q1 - 1.5 * IQR 미만, Q3 + 1.5 * IQR 초과를 이상치로 간주
        
        ![image.png](assets/img/posts/study/AI/데이터 품질 문제/image 3.png)

3. **Isolation Forest**
    - 결정 트리(decision tree) 계열의 비지도 학습 알고리즘으로 고차원 데이터셋에서 이상치를 탐지할 때 효과적
    - 데이터의 **“고립”**을 기반으로 작동하며 이상치는 정상 데이터보다 고립되기 쉽다는 점을 활용
    - 작동 원리
        1. 데이터를 랜덤한 방식으로 나무 구조로 분할
        2. 이상치는 적은 횟수의 분할만으로 고립되므로 고립에 필요한 경로 길이가 짧음
        3. 여러 나무의 결과를 평균 내어 이상치 점수를 계산함
    
    ![https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/05/illustration-isolation-forest.jpg?resize=1024%2C576&ssl=1](https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/05/illustration-isolation-forest.jpg?resize=1024%2C576&ssl=1)
    
4. **DBScan(Density-Based Spatial Clustering of Applications with Noise)**
    - 밀도 기반 클러스터링 알고리즘으로, 밀도 기준으로 군집을 정의하고 밀도가 낮은 데이터를 이상치로 탐지
    - 밀집된 지역에서 데이터를 군집화한 후, 밀도가 낮아 어느 군집에도 속하지 못하는 점들을 이상치로 간주
    - 주요 매개변수
        - Eps : 데이터 포인트가 이웃이라고 간주되는 거리의 반경으로, 데이터 분포에 따라 결정
        - MinPts : 한 군집을 이루는 최소 데이터 포인트 개수로, 데이터 차원(D)에 따라 일반적으로 D+1 이상의 값으로 설정함
    
    ![image.png](assets/img/posts/study/AI/데이터 품질 문제/image%201.png)
    

### **불균형 데이터(Imbalanced Data)**

- 범주형 데이터 중, **클래스 간 샘플 수의 비율이 크게 차이 나는 데이터**
- 문제점
    - 다수의 클래스에 맞춰 예측하는 경향이 생김
    - 정확도가 높더라도 소수 클래스 예측 성능은 낮을 수 있음
    - 소수 클래스의 정보를 손실하여 정확한 예측이 어려움
- 예시
    - 이진 분류에서 긍정 클래스가 10%고, 부정 클래스가 90%인 경우

#### **해결 방법**

1. **OverSampling**
    - 소수 클래스 데이터를 복제하거나 새로운 샘플을 생성하여 클래스 비율을 맞추는 방법
    - SMOTE(Synthetic Minority Over-sampling Technique)를 활용해, 기존 데이터를 바탕으로 소수 클래스의 가상데이터를 생성
    - 주의점
        - 데이터를 복제하거나 생성할 때 과적합 위험 증가
        - 데이터 크기가 커지면 학습 시간이 길어짐
2. **UnderSampling**
    - 다수 클래스 데이터를 줄여 클래스 비율을 맞추는 방법
    - 데이터가 너무 많아 불필요한 데이터를 줄이거나 샘플 간 대표성을 유지할 때 사용
    - 주의점
        - 데이터 손실로 정보 유실 가능
        - 다수 클래스에 중요한 패턴을 놓칠 위험 존재
3. **클래스 가중치 부여**
    - 소수 클래스에 높은 가중치를 부여하여 모델이 소수 클래스에 주의를 기울이도록 유도
    - 가중치는 클래스 비율에 따라 자동 계산되거나 직접 설정할 수 있음
    - 주의점
        - 파라미터 설정이 필요하며, 가중치 설정이 정확하지 않으면 성능이 떨어질 수 있음
        - 일부 모델에 따라 사용 제한
4. **앙상블 기법**
    - 다수의 분류기법을 결합하여 개별 분류기의 약점을 보완하고 예측 성능을 높이는 기법
    - 주의점
        - 계산 비용이 높아 대규모 데이터셋에서 속도 저하
        - 모델 해석력이 낮아 결과를 설명하기 어려움
    - 모델 종류
        - RandomForest
        - XGBoost
        - LightGBM

### **데이터 스케일링**

- 데이터의 크기, 단위, 분포 차이를 조정하여 머신러닝 모델의 성능을 향상시키기 위한 과정
- 많은 알고리즘은 변수의 크기에 영향을 받기 때문에 값이 큰 데이터가 더 큰 영향을 미치지 않도록 조정 필요
- 단위나 범위가 크게 다를 경우, 특정 특징이 다른 특징을 압도하는 문제를 방지

#### **종류**

1. **Standard Scaler**
    - 표준화 방식
    - 데이터의 평균을 0, 표준편차를 1로 조정
    - $z = \frac{x - \mu}{\sigma}$
2. **Min-Max Scaler**
    - 정규화 방식
    - 데이터 값을 0과 1 사이의 값으로 스케일링
    - $x' = \frac{x - x_{min}}{x_{max} - x_{min}}$
3. **Log 변환**
    - 데이터의 분포가 심하게 왜곡된 경우 로그 적용
    - 음수나 0 값은 변환 불가
    - $x' = \log(x + 1)$

## **자연어 데이터**

### **문제점**

- **비정형 데이터**: 틀이 잡혀 있지 않고 사전 정의가 없는 데이터
- **희소성**: 높은 차원에 비해 특정 단어가 드물게 나타나, 문장 간 유사도를 파악하기 어려움
- **모호성과 중의성**: 단어의 다의성과 문맥 의존성
    - (예: "Apple": 과일 vs 회사)
- **다양성**: 동일한 의미를 가진 다양한 표현
    - (예: 좋아요 -> 좋아, 굿, nice)
- **비표준 표현**: 대/소문자, 띄어쓰기, 맞춤법, 오탈자, 문법 오류
- **불필요한 단어**: 길이가 짧은 단어, 등장빈도가 적은 단어, 특수기호
- **비구조화**: 컴퓨터가 자연어 데이터를 인식 및 처리할 수 없음

### **문제 해결 방법**

1. **텍스트 정제(Text Cleaning)**
    - 텍스트 데이터에서 의미가 없는 기호, 숫자, 불용어 등을 제거
        - 불용어 : "the", "is", "것", "들"와 같이 의미가 없는 단어를 뜻함
    - 예시
        - URL, HTML 태그 제거
2. **텍스트 정규화(Text Normalization)**
    - 데이터를 일관된 형식으로 변환하여 모델이 학습하기 쉽도록 만듦
    - 구어체, 약어, 이모티콘 등 다양한 표현을 문법적, 의미적 표준 형태로 통일함
    - 예시
        - 소문자 변환
        - 철자 교정
    
    | **정규표현식** | **설명** |
    | --- | --- |
    | [0-9] | 모든 숫자 |
    | [a-zA-Z] | 모든 알파벳 |
    | \d | 숫자 매칭, [0-9] 동일표현 |
    | \D | 숫자가 아닌 것과 매칭 |
    | [^xy] | ^는 not 표현, 즉 x,y를 제외 |
    
    ```python
    import re
    
    def text_normalization(text):
        # 1. 소문자로 변환
        text = text.lower()
        # 2. HTML 태그 제거
        text = re.sub(r'<.*?>', '', text)
        # 3. URL 제거
        text = re.sub(r'http\\S+|www\\S+', '', text)
        # 4. 숫자 제거
        text = re.sub(r'\\d+', '', text)
        # 5. 특수문자 제거
        text = re.sub(r'[^\\w\\s]', '', text)
        # 6. 불필요한 공백 제거
        text = re.sub(r'\\s+', ' ', text).strip()
        return text
    
    ```
1. **토큰화(Tokenization)**
    - 텍스트를 단어, 문장, 서브워드 등으로 분리하여 **분석이 가능한 형태로 만듦**
    - 텍스트 분석에서 다룰 수 있는 최소 단위로 구조화하여 모델이 이해할 수 있게 함
    - 라이브러리
        - transformers
        - konlpy(한국어 적용)
        
        | **종류** | **문장** | **결과** |
        | --- | --- | --- |
        | 단어 단위 | "I love NLP!" | ["I", "love", "NLP!"] |
        | 문장 단위 | "I love NLP. It's amazing!" | ["I love NLP.", "It's amazing!"] |
        | 서브워드 | "unhappiness" | ["un", "happi", "ness"] |

2. **텍스트 벡터화(Text Vectorization)**
    - 자연어 데이터를 수치화하여 컴퓨터가 처리할 수 있는 형태로 변환

    | **벡터화 방식** | **특징** | **장점** | **단점** |
    | --- | --- | --- | --- |
    | Bag-of-Words | 단어의 단순 빈도를 기반 | 간단하고 구현 용이 | 문맥 정보 부족
    고차원 희소 행렬 생성 |
    | TF-IDF | 단어 빈도와 역문서 빈도로 가중치를 부여 | 단어의 중요도 반영
    불필요한 단어 제거 | 문맥 정보 부족 |
    | Word2Vec/GloVe | 단어를 밀집 벡터로 표현하여 단어 간 관계를 포함 | 단어 간 유사성 표현 가능 | 문맥 정보 부족
    문장 수준 표현 어려움 |
    | BERT/GPT | 문맥적 의미를 반영한 벡터화 | 문맥과 순서를 고려 | 계산 비용이 높음
    고사양 하드웨어 필요 |


    1. **TF-IDF(Term Frequency-Inverse Document Frequency)**
      - 텍스트 데이터에서 특정 단어의 중요도를 평가하기 위한 통계적 방법
      - 단어의 빈도(TF)와 역문서 빈도(IDF)를 조합하여 가중치를 부여
      - 수식 :  $TF-IDF(t, d, D) = \frac{f(t, d)}{ N_d} \times \log \frac{N}{1+n(t)}$
        - $f(t, d)$ : 단어 t가 문서 d에서 등장한 횟수
        - $N_d$ : 문서 d의 전체 단어 수
        - $N$ : 전체 문서의 수
        - $n(t)$ : 단어 t가 등장한 문서의 수
    2. **Word2Vec**
        - 단어를 고차원 벡터로 매핑하는 임베딩 방법으로, 단어의 **문맥적 관계를 학습함**
        - 단어 간 수학적 연산 가능
        - 의미적 유사성 반영
            - 예시 : king - man + woman = queen
        - CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 모델 구조를 사용하여 효율적 학습 가능
            
            ![image.png](assets/img/posts/study/AI/데이터 품질 문제/image%202.png)

            - CBOW(Continuous Bag of Words)
                - 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하며, 중심 단어가 회귀 단어일 때 유리함
                
                ![https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)
                
                ![https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)
                
            - Skip-Gram
                - 중심 단어에서 주변 단어를 예측하며, 주변 단어가 희귀 단어일 때 유리함
                
                ![https://wikidocs.net/images/page/22660/skipgram_dataset.PNG](https://wikidocs.net/images/page/22660/skipgram_dataset.PNG)
                
                ![https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)
                

## Reference

1. [https://yozm.wishket.com/magazine/detail/1919/](https://yozm.wishket.com/magazine/detail/1919/)
2. [https://slideplayer.com/slide/6394283/](https://slideplayer.com/slide/6394283/)
3. [https://www.kdnuggets.com/2019/11/understanding-boxplots.html](https://www.kdnuggets.com/2019/11/understanding-boxplots.html)
4. [https://spotintelligence.com/2024/05/21/isolation-forest/](https://spotintelligence.com/2024/05/21/isolation-forest/)
5. [https://www.researchgate.net/figure/Density-based-spatial-clustering-of-applications-with-noise-DBSCAN-concept_fig3_331324525](https://www.researchgate.net/figure/Density-based-spatial-clustering-of-applications-with-noise-DBSCAN-concept_fig3_331324525)
6. [https://medium.com/@fraidoonomarzai99/word2vec-cbow-skip-gram-in-depth-88d9cc340a50](https://medium.com/@fraidoonomarzai99/word2vec-cbow-skip-gram-in-depth-88d9cc340a50)
7. [https://wikidocs.net/22660](https://wikidocs.net/22660)