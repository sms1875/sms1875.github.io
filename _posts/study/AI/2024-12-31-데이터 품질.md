---
layout: post
title: "데이터 품질"
date: 2024-12-31 07:56:00+0900
categories: [Study, AI]
tags: [ML, DL, Data]
math: true
---
## **데이터 품질**

### **데이터의 증가**

- 2000년대 초반 ~ 2023년까지 생산한 데이터의 양이 90ZB에 달함
- 이는 그 이전 5000년간 쌓은 데이터의 3000배가 넘는 수치
- 1인당 일주일에 1TB 용량의 데이터를 생성하는 것과 같은 상황

![https://dimg.donga.com/wps/NEWS/IMAGE/2022/08/24/115122834.1.jpg](https://dimg.donga.com/wps/NEWS/IMAGE/2022/08/24/115122834.1.jpg)

### **데이터 품질의 정의**

- 데이터 유형(정형, 비정형)에 따라 다르다
- 일반적인 데이터 품질 기준의 정의
- AI 모델이 학습하기에 얼마나 완전하고 정확하게 구축되었는지 나타냄

1. **완전성 (Completeness)**
    - 필수 항목에 누락이 없어야 함
    - 지켜지지 않은 경우 : AI 모델이 학습할 수 있는 정보가 줄어들고 중요한 정보가 손실될 수 있음
2. **유일성 (Uniqueness)**
    - 데이터 항목은 정해진 데이터 유효 범위 및 도메인을 충족해야 함
    - 지켜지지 않은 경우
        - 중복된 특정 패턴을 과대평가하여 불균형한 모델이 만들어짐
        - 불필요한 학습 시간과 연산 자원의 낭비 발생
3. **유효성 (Validity)**
    - 데이터 항목은 정해진 데이터 유효 범위 및 도메인을 충족해야 함
    - 지켜지지 않은 경우
        - 비현실적인 결과를 출력 (예: 기대수명 200세)
4. **일관성 (Consistency)**
    - 데이터가 지켜야 할 구조, 값, 표현되는 형태가 일관되게 정의되고, 일치해야 함
    - 지켜지지 않은 경우
        - 모델이 서로 상충되는 데이터를 학습하여 혼란이 가중될 수 있음
        - 편향된 결과를 도출하거나 일반화 성능 저하 발생
5. **정확성 (Accuracy)**
    - 실제 존재하는 객체의 표현 값이 정확하게 반영되어야 함
    - 지켜지지 않은 경우
        - 모델이 잘못된 패턴을 학습하여 신뢰성이 떨어지는 결과를 출력
        - 윤리적 혹은 법적 문제 발생 가능
6. **적시성 (Timeliness)**
    - 데이터가 최신 정보를 반영해야 함
    - 지켜지지 않은 경우
        - 주식 예측 모델 등에서 데이터가 오래되어 최신 정보를 반영하지 못하면, 부정확한 결과를 출력할 수 있음
        - 예: 동일 기간의 주가 데이터라도 오래된 정보는 현재 상황을 반영하지 못함

### **데이터 품질 기준 표**

| 품질 기준  | 세부 품질 기준     | 품질 기준 설명                                                               | 활용 예시                                                        |
| ---------- | ------------------ | ---------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **완전성** | 개별 완전성        | 필수 컬럼에는 누락 값이 없어야 함                                            | 고객의 아이디는 NULL일 수 없음                                   |
|            | 조건 완전성        | 조건에 따라 컬럼 값이 항상 존재해야 함                                       | 기업 고객의 등록번호가 NULL일 수 없음                            |
| **유일성** | 단독 유일성        | 컬럼은 유일한 값을 가져야 함                                                 | 고객의 이메일 주소는 유일하게 존재                               |
|            | 조건 유일성        | 조건에 따른 컬럼 값은 유일해야 함                                            | 강의 시작일에 강의실 코드, 강사 코드가 동일한 값은 유일하게 존재 |
| **유효성** | 범위 유효성        | 값이 주어진 범위 내 존재해야 함                                              | 수능 시험의 점수는 0 이상 100 이하의 값                          |
|            | 날짜 유효성        | 날짜 유형은 유효한 날짜 값을 가져야 함                                       | 20250231은 유효하지 않은 값                                      |
|            | 형식 유효성        | 정해진 형식과 일치하는 값을 가져야 함                                        | 이메일 형식은 xxx@xxx의 형식                                     |
| **일관성** | 포맷 일관성        | 동일 유형의 값은 형식이 일치해야 함                                          | 날짜는 YYYYMMDD 형식으로 통일                                    |
|            | 참조 무결성        | 여러 값이 참조 관계에 있으면 그 무결성을 유지해야 함                         | 대출 번호는 대출 상세 내역에 존재해야 함                         |
|            | 데이터 흐름 일관성 | 데이터를 생성하거나 가공하여 이동하는 경우, 연관된 데이터는 모두 일치해야 함 | 현재 가입 고객 수와 DW의 고객 수는 일치해야 함                   |
| **정확성** | 선후관계 정확성    | 여러 컬럼의 값이 선후관계에 있으면 관련 규칙을 지켜야 함                     | 시작일은 종료일 이전 시점에 존재                                 |
|            | 계산/집계 정확성   | 특정 컬럼의 값이 여러 컬럼의 계산된 값이면 그 계산 결과가 정확해야 함        | 월 매출액은 일 매출액의 총합과 일치                              |
|            | 최신성             | 정보 수집, 갱신 주기를 유지                                                  | 고객의 현재 주소 데이터는 마지막으로 이사한 곳의 주소와 동일     |
| **적시성** | 최신 데이터        | 모델이 최신 데이터를 기반으로 학습할 수 있어야 함                            | 주식 예측 데이터는 최신 주가 정보를 반영해야 함                  |

### **최소 품질의 기준**

- 데이터의 양이 많을 경우 모든 품질 기준 항목을 완벽히 맞추기는 불가능에 가까움
- 비율을 정해 최소 품질 달성 기준을 설정하는 것도 좋은 방법

| 항목       | 최소 기준                  | 검토 방법                                   |
| ---------- | -------------------------- | ------------------------------------------- |
| **완전성** | 결측 값 비율 < 5%          | 결측 값 탐지 및 비율 계산                   |
| **유효성** | 데이터 규칙 위반 < 5%      | 정규식 및 포맷 검증                         |
| **정확성** | 참조 데이터와 일치율 > 90% | 정규식 및 포맷 검증 외부 데이터 소스와 비교 |
| **유일성** | 중복 값 비율 < 5%          | 고유 식별 필드 설정 및 검증                 |
| **일관성** | 데이터 간 불일치율 < 3%    | 데이터베이스 참조 무결성 검증               |

### **비정형 데이터의 품질**
- 이미지, 동영상, 오디오 등 정형화되지 않은 데이터
- 각 데이터 유형 별로 품질 기준을 정의하는 것이 모두 다름

#### **이미지 데이터**

| 품질 기준  | 품질 기준 설명                         | 활용 예시                                                     |
| ---------- | -------------------------------------- | ------------------------------------------------------------- |
| **해상도** | 이미지의 픽셀 수                       | 고해상도 이미지를 위해 QHD 이상의 해상도만 채택               |
| **사용성** | 사용자에게 친숙하고 사용이 용이한 포맷 | jpg, png 포맷만 사용 (tif, dng 등은 친숙하지 않음)            |
| **선명도** | 이미지 내 경계의 선명함의 정도         | Laplacian Variance 알고리즘 적용하여 측정                     |
| **이해성** | 이미지가 정보를 명확히 전달하는 정도   | AI 모델이 이미지를 인식하는 정확도가 90% 이상인 데이터만 채택 |

#### **오디오 데이터**

| 품질 기준       | 품질 기준 설명                    | 활용 예시                                                          |
| --------------- | --------------------------------- | ------------------------------------------------------------------ |
| **잡음비**      | 유용 신호와 배경 노이즈 비율 측정 | SNR(신호 대 잡음비) 10dB 이하(통화 녹음 정도의 음질) 데이터만 선정 |
| **동적 범위**   | 가장 큰 소리와 작은 소리의 범위   | Dynamic Range 60~90dB (일반적인 대화 정도의 소음)                  |
| **길이 일관성** | 재생 시간 길이의 일관성           | 55초~65초 범위의 오디오                                            |
| **주파수 범위** | 특정 주파수 범위 포함 여부        | 사람이 들을 수 있는 소리 기준은 20Hz~20kHz 범위 내 주파수 포함     |

### **나쁜 데이터, 저품질 데이터**

- 데이터 품질 정의 기준에 맞지 않는 데이터들
- 이러한 데이터는 AI 모델 학습에 악영향을 미침
    - *Garbage in, Garbage out*

| 품질 기준  | 내용                                  |
| ---------- | ------------------------------------- |
| **완전성** | 데이터 누락 및 결측치 존재            |
| **유일성** | 중복 데이터 존재                      |
| **유효성** | 범위를 벗어나거나 정의와 맞지 않는 값 |
| **일관성** | 단위 불일치 등 비일관적인 형식과 구조 |
| **정확성** | 잘못된 정보가 있음                    |

#### **저품질 데이터가 AI 모델에 미치는 영향**

- **라벨링이 잘못된 데이터**는 AI에 혼동을 줌
    - 이로 인해 이미지 분류 성능이 매우 저하됨
- **편향된 데이터**로 인해 편향이 발생할 수 있음
    - **결과적으로 신뢰도와 공정성이 하락**하게 됨
- 예시
    - 잘못된 데이터로 학습한 분류 모델
        1. **학습 데이터**
            - 라벨링이 잘못된 이미지들이 포함되어 있음
        2. **학습 결과**
            - 모델이 잘못된 예측을 수행함 (예: 고양이를 제대로 분류하지 못함)
    - 편향된 AI의 판단
        1. **상황**
            - 금발이라는 이유로 특정 범죄와 관련지어 판단한 AI
        2. **문제점**
            - 데이터 편향으로 인해 잘못된 판단을 내림
            - 사회적 신뢰도 하락과 공정성 문제를 초래

![image.png](assets/img/posts/study/AI/데이터 품질/ETH_2_01_00_A_01_v1.jpg)

---

## **데이터의 품질 관리**

### **데이터 품질 관리의 중요성**

- 데이터의 양이 많다 보니 좋은 데이터를 ㅅ너별해야 함
- 보고서에 따르면, 데이터 품질 저하로 인해 조직은 매년 1,290만 달러의 비용을 지출
- AI 시스템에서 데이터가 차지하는 비중이 크기에 품질 관리에 신경을 써야 함
- 지속적인 관리 필요

![image.png](assets/img/posts/study/AI/데이터 품질/image0.png)

#### **데이터 품질 관리**

- 데이터 활용에 있어 수집, 저장, 분석 활용 단계별로 데이터 품질 관리를 위한 일반적인 점검 사항

| 단계            | 점검 사항                                               |
| --------------- | ------------------------------------------------------- |
| **데이터 수집** | 수집 기준의 타당성(근거, 통계적 유의성)이 확보되었는가? |
|                 | 추출 조건에 맞는 정보와 관련 항목 모두가 추출되었는가?  |
|                 | 악의적 유포 데이터 제거 방법을 확보하였는가?            |
| **데이터 저장** | 누락된 데이터는 없는가?                                 |
|                 | 저장을 위한 구조의 적절성이 확보되었는가?               |
| **데이터 분석** | 최신 데이터인가?                                        |
|                 | 모델에 필요한 충분한 정보가 제공되고 있는가?            |

### **품질 측정 및 지수**

1. **측정 항목 작성**
    - **측정 기준의 선정 범위**는 품질 관리 정책, 목적, 방향에 의해 달라질 수 있음
    - **비정형 데이터**에 대한 측정 항목은 데이터 유형(이미지, 비디오 등)에 따라 달라질 수 있음
    
    | 일반적인 데이터                                | 이미지 데이터  | GIS 데이터     |
    | ---------------------------------------------- | -------------- | -------------- |
    | 완전성, 유효성, 정확성, 유일성, 일관성, 적시성 | 해상도, 이해성 | 운용성, 준수성 |

2. **품질 측정 및 품질 지수 산출**
    - 품질 측정 기준별 점수를 매긴 뒤, 기준별 중요도에 따라 품질 지수를 계산
    - 품질 기준별 낮은 점수를 빠르게 파악하고 대응 가능
    - **공식**
      - **측정 기준별 점수**: $Q_s = \frac{\Sigma R_i \times 20}{C_q}$
        - $\Sigma R_i$: 측정 기준별 측정 결과 점수의 합
        - $C_q$: 측정 기준별 측정 문항 수
      - **품질 지수**: $QI = \Sigma (Q_s \times W_{tb})$
        - $W_{tb}$: 해당 콘텐츠 유형의 측정 기준별 가중치
     
3. **품질 측정 및 품질 지수 산출** 
    - **계산 예시 (100점 만점 기준)**
    
    <div style="overflow-x: auto;">
    <table>
      <thead>
        <tr>
          <th>측정 기준</th>
          <th>문항 수</th>
          <th>문항별 점수</th>
          <th>가중치</th>
          <th>계산</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>완전성</td>
          <td>3</td>
          <td>5, 4, 4</td>
          <td>0.4</td>
          <td>\( Q_s = \frac{(5+4+4)\times20}{3} = 86.67 \)</td>
        </tr>
        <tr>
          <td>정확성</td>
          <td>4</td>
          <td>5, 5, 4, 3</td>
          <td>0.3</td>
          <td>\( Q_s = \frac{(5+5+4+3)\times20}{4} = 85.0 \)</td>
        </tr>
        <tr>
          <td>일관성</td>
          <td>2</td>
          <td>4, 3</td>
          <td>0.3</td>
          <td>\( Q_s = \frac{(4+3)\times20}{2} = 70.0 \)</td>
        </tr>
      </tbody>
    </table>
    </div>
    
    - **품질 지수 계산**
      
        $QI = (86.67 \times 0.4) + (85.0 \times 0.3) + (70.0 \times 0.3) = 81.168$
        
4. **오류율 측정**
    - **정형 데이터**는 대량 데이터 중 일부 오류의 비율로 측정
    - **비정형 데이터**는 정형 데이터 대비 오류 비중이 작아 참고 정도로만 활용
    - **공식**
      - **측정 내용별 오류율**: $e_i = \frac{c_e}{c_t}$
        - $c_e$: 오류 건수
        - $c_t$: 총 건수
      - **측정 내용별 가중 오류율**: $e_{wi} = e_i \times W_{tqi}$
        - $W_{tqi}$: 측정 항목 중요도
      - **가중 평균 오류율**: $E = \frac{\Sigma_{i=1}^n e_{wi}}{\Sigma_{i=1}^n W_{tqi}}$
  
5. **오류율 측정 - 계산 예시**
    
    
    <div style="overflow-x: auto;">
    <table>
      <thead>
        <tr>
          <th>항목</th>
          <th>오류 건수</th>
          <th>총 건수</th>
          <th>중요도</th>
          <th>오류율 \( e_i \)</th>
          <th>가중 오류율 \( e_{wi} \)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>이름</td>
          <td>5</td>
          <td>500</td>
          <td>0.5</td>
          <td>\( 0.01 \)</td>
          <td>\( 0.005 \)</td>
        </tr>
        <tr>
          <td>나이</td>
          <td>20</td>
          <td>400</td>
          <td>0.3</td>
          <td>\( 0.05 \)</td>
          <td>\( 0.015 \)</td>
        </tr>
        <tr>
          <td>주소</td>
          <td>15</td>
          <td>300</td>
          <td>0.2</td>
          <td>\( 0.05 \)</td>
          <td>\( 0.01 \)</td>
        </tr>
      </tbody>
    </table>
    </div>

    - **가중 평균 오류율 계산**

      $E = \frac{0.005 + 0.015 + 0.01}{0.5 + 0.3 + 0.2} = 0.03$
        
    - **결론**
        - 가중 평균 오류율을 0.01 이하로 낮추기 위해 데이터 개선 필요

### 좋은 데이터의 예시

#### 정형 데이터

1. **Iris**
    - 붓꽃(iris)의 품종을 분류한 데이터셋
    - 완전성, 유효성, 정확성, 유일성, 일관성의 측면을 모두 살펴봤을 때 품질이 높음
    
    ```
    sepal_length  sepal_width  petal_length  petal_width  species
    5.1           3.5          1.4           0.2          setosa
    4.9           3.0          1.4           0.2          setosa
    ```
    
2. **NYC Taxi & Limousine Commission**
    - 뉴욕의 택시 서비스 데이터를 기반으로 한 데이터셋
    - 시간, 거리, 요금 등의 데이터가 포함되어 있음
    
    ```
    tpepPickupDatetime   tpepDropoffDatetime   passengerCount   tripDistance
    1/24/2020 5:43:21 PM 1/24/2020 5:48:17 PM 1               1.05
    1/24/2020 10:25:49 AM 1/24/2020 10:30:57 AM 2             0.76
    ```
    
3. **Wine Quality**
    - 와인의 특성을 기반으로 와인을 평가한 데이터
    - 모든 값을 float로 표현하며, 일관성에서 특히 우수
    
    ```
    fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides
    7.4           0.7              0.0          1.9             0.076
    7.8           0.88             0.0          2.6             0.098
    ```
    
4. **Titanic**
    - 타이타닉호 침몰 사고에서 승객들의 정보가 담긴 데이터
    - 품질 기준을 따져봤을 때 결측치가 다수 있는 등 품질이 높지는 않으나, 품질 기준을 체크하기 용이해 교육용으로 좋은 데이터
    
    ```
    survived  pclass  sex     age  sibsp  parch  fare     embarked  class
    0         3       male    22.0 1      0      7.2500   S         Third
    1         1       female  38.0 1      0      71.2833  C         First
    ```
    

#### 비정형 데이터

1. **NQ (Natural Question)**
    - 구글에서 개발한 자연어 질의응답 데이터셋
    
    ![https://mitp.silverchair-cdn.com/mitp/content_public/journal/tacl/7/10.1162_tacl_a_00276/13/m_acl00276f01l.png?Expires=1737550153&Signature=GCQbN7TelO6~LXQ8tuTth-VlTAQNiqp92gUIMdRKMsu5PsNOlXwK9M8L1LE0RTKMIv9ZlyEZ0XqW28LCWowVkbwAr9u2F0V4TZgz~zTB8PLiV2FKN8cBiX5REi197bcoPLIp1ln1F0sQkHiC0xQfKbplUIaXMVEtd4vDLuJ7aZf8KiPJvcB9MS6qwVmauh0yHHzsAxdAqBhUxE3hGCTa6LVHy4lebp1z8R4CURg9opjlLBahJOF6wrhrXXboHRNc48l8ILW824uF6~Fuk2ak7H~vGNfmhWXO4h4z5vEF7xsfa1~VhmXQv8gULKxfARokVRaGdexoxvVItpNK0Ex2XQ__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA](https://mitp.silverchair-cdn.com/mitp/content_public/journal/tacl/7/10.1162_tacl_a_00276/13/m_acl00276f01l.png?Expires=1737550153&Signature=GCQbN7TelO6~LXQ8tuTth-VlTAQNiqp92gUIMdRKMsu5PsNOlXwK9M8L1LE0RTKMIv9ZlyEZ0XqW28LCWowVkbwAr9u2F0V4TZgz~zTB8PLiV2FKN8cBiX5REi197bcoPLIp1ln1F0sQkHiC0xQfKbplUIaXMVEtd4vDLuJ7aZf8KiPJvcB9MS6qwVmauh0yHHzsAxdAqBhUxE3hGCTa6LVHy4lebp1z8R4CURg9opjlLBahJOF6wrhrXXboHRNc48l8ILW824uF6~Fuk2ak7H~vGNfmhWXO4h4z5vEF7xsfa1~VhmXQv8gULKxfARokVRaGdexoxvVItpNK0Ex2XQ__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)
    
2. **ImageNet**
    - 1,000여 개의 카테고리로 분류된 1,400만 개 이상의 대규모 이미지 데이터셋
    
    ![https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k_icon.jpg](https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k_icon.jpg)

---
    
## 데이터 품질 문제 유형

### **결측치(Missing Value)**

- Null 값
- 수집된 데이터셋 중 **관측되지 않거나 누락된 데이터**
- 발생 시 잘못된 분석 결과를 낳거나 분석 시 에러가 발생할 수 있음
- 원인
    - 센서 오작동, 네트워크 문제 등
    - 설문조사에서 응답자가 특정 질문에 답하지 않는 경우
    - 데이터를 입력하는 과정에서 값이 누락되거나 잘못 기록
    - 데이터베이스 손상, 저장 프로세스의 오류
- 예시
    
    
    | **ID** | **날짜(Date)** | **온도(Temperature)** | **습도(Humidity)** | **풍속(Wind Speed)** | **비고(Notes)**         |
    | ------ | -------------- | --------------------- | ------------------ | -------------------- | ----------------------- |
    | 1      | 2024-12-01     | 15.5                  | 45                 | 3.2                  | 정상                    |
    | 2      | 2024-12-02     | -                     | 50                 | 4.1                  | 온도 센서 오류          |
    | 3      | 2024-12-03     | 16.0                  | -                  | 2.9                  | 습도 데이터 누락        |
    | 4      | 2024-12-04     | 17.2                  | 48                 | -                    | 풍속 데이터 미기록      |
    | 5      | 2024-12-05     | -                     | -                  | -                    | 기록 과정에서 문제 발생 |
    | 6      | 2024-12-06     | 14.3                  | 42                 | 3.0                  | 정상                    |

#### **종류**

1. **MCAR(Missing Completely At Random, 완전 무작위 결측)**
    - 결측치가 완전히 랜덤하게 발생
    - 결측 여부가 데이터 내 모든 변수와 상관없음
    - 데이터가 충분히 크다면 분석 결과에 영향을 미치지 않음
    - 해결 방법
        - 결측치 제거
        - 평균, 중앙값 등으로 대체
    - 예시
        - 센서 오류 및 네트워크 장애로 일부 데이터 유실
2. **MAR(Missing At Random, 무작위 결측)**
    - 결측 여부가 관측된 다른 변수에 의해 설명가능
    - 결측치 자체는 랜덤이 아니지만, 관측된 데이터로 예측가능
    - 해결 방법
        - 관측된 변수 기반 예측 모델 활용(회귀, KNN)
        - 분포 기반 대체(조건부 평균)
    - 예시
        - 환자의 건강 기록 중 약물 사용 정보 누락, 다른 건강 지표로 추측 가능
3. **MNAR(Missing Not At Random, 비 무작위 결측)**
    - 결측 여부가 데이터 내 관측되지 않은 변수와 관련
    - 결측 자체가 특정 이유로 발생하며, 그 원인을 찾기 쉽지 않음
    - 해결 방법
        - 도메인 지식을 통한 결측 원인 파악 후 대체
        - 결측치 자체를 새로운 범주로 설정(미응답)
    - 예시
        - 응답자들이 고의로 사실과 다른 응답을 하는 경우(소득범위, 우울증)

### **이상치(Outlier)**

- 관측된 데이터의 **범위에서 많이 벗어난 아주 작은 값이나 큰 값**
- 데이터 분석 혹은 모델링에서 의사결정에 큰 영향을 미침
- 원인
    - 잘못된 데이터 입력 또는 장비 오류
    - 데이터 분포 내에서 극단적인 값 발생
    - 데이터 수집에서 샘플링 과정에서 발생한 오류

![https://yozm.wishket.com/media/news/1919/image001.png](https://yozm.wishket.com/media/news/1919/image001.png)

#### **탐지 방법**

1. **Z-Score**
    - 데이터의 분포가 정규 분포를 이룰 때, 데이터의 표준 편차를 이용해 이상치를 탐지하는 방법
    - 해당 데이터가 평균으로부터 얼마나 표준편차 만큼 벗어나 있는지 의미
    - 주로 3 이상, -3이하 값을 이상치로 판별함
        
        ![image.png](assets/img/posts/study/AI/데이터 품질/image.png)
        
2. **IQR(Interquartile Range) with Box plots**
    - 데이터의 분포가 정규를 이루지 않거나 한쪽으로 치우친 경우 IQR 값을 사용해 이상치를 탐지하는 방법
    - 박스 플롯을 통해 최소값, 최대값, 중앙값, Q1, Q3 값을 알 수 있음
    - Q1 - 1.5 * IQR 미만, Q3 + 1.5 * IQR 초과를 이상치로 간주
        
        ![image.png](assets/img/posts/study/AI/데이터 품질/image 3.png)

3. **Isolation Forest**
    - 결정 트리(decision tree) 계열의 비지도 학습 알고리즘으로 고차원 데이터셋에서 이상치를 탐지할 때 효과적
    - 데이터의 **“고립”**을 기반으로 작동하며 이상치는 정상 데이터보다 고립되기 쉽다는 점을 활용
    - 작동 원리
        1. 데이터를 랜덤한 방식으로 나무 구조로 분할
        2. 이상치는 적은 횟수의 분할만으로 고립되므로 고립에 필요한 경로 길이가 짧음
        3. 여러 나무의 결과를 평균 내어 이상치 점수를 계산함
    
    ![https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/05/illustration-isolation-forest.jpg?resize=1024%2C576&ssl=1](https://i0.wp.com/spotintelligence.com/wp-content/uploads/2024/05/illustration-isolation-forest.jpg?resize=1024%2C576&ssl=1)
    
4. **DBScan(Density-Based Spatial Clustering of Applications with Noise)**
    - 밀도 기반 클러스터링 알고리즘으로, 밀도 기준으로 군집을 정의하고 밀도가 낮은 데이터를 이상치로 탐지
    - 밀집된 지역에서 데이터를 군집화한 후, 밀도가 낮아 어느 군집에도 속하지 못하는 점들을 이상치로 간주
    - 주요 매개변수
        - Eps : 데이터 포인트가 이웃이라고 간주되는 거리의 반경으로, 데이터 분포에 따라 결정
        - MinPts : 한 군집을 이루는 최소 데이터 포인트 개수로, 데이터 차원(D)에 따라 일반적으로 D+1 이상의 값으로 설정함
    
    ![image.png](assets/img/posts/study/AI/데이터 품질/image%201.png)
    

### **불균형 데이터(Imbalanced Data)**

- 범주형 데이터 중, **클래스 간 샘플 수의 비율이 크게 차이 나는 데이터**
- 문제점
    - 다수의 클래스에 맞춰 예측하는 경향이 생김
    - 정확도가 높더라도 소수 클래스 예측 성능은 낮을 수 있음
    - 소수 클래스의 정보를 손실하여 정확한 예측이 어려움
- 예시
    - 이진 분류에서 긍정 클래스가 10%고, 부정 클래스가 90%인 경우

#### **해결 방법**

1. **OverSampling**
    - 소수 클래스 데이터를 복제하거나 새로운 샘플을 생성하여 클래스 비율을 맞추는 방법
    - SMOTE(Synthetic Minority Over-sampling Technique)를 활용해, 기존 데이터를 바탕으로 소수 클래스의 가상데이터를 생성
    - 주의점
        - 데이터를 복제하거나 생성할 때 과적합 위험 증가
        - 데이터 크기가 커지면 학습 시간이 길어짐
2. **UnderSampling**
    - 다수 클래스 데이터를 줄여 클래스 비율을 맞추는 방법
    - 데이터가 너무 많아 불필요한 데이터를 줄이거나 샘플 간 대표성을 유지할 때 사용
    - 주의점
        - 데이터 손실로 정보 유실 가능
        - 다수 클래스에 중요한 패턴을 놓칠 위험 존재
3. **클래스 가중치 부여**
    - 소수 클래스에 높은 가중치를 부여하여 모델이 소수 클래스에 주의를 기울이도록 유도
    - 가중치는 클래스 비율에 따라 자동 계산되거나 직접 설정할 수 있음
    - 주의점
        - 파라미터 설정이 필요하며, 가중치 설정이 정확하지 않으면 성능이 떨어질 수 있음
        - 일부 모델에 따라 사용 제한
4. **앙상블 기법**
    - 다수의 분류기법을 결합하여 개별 분류기의 약점을 보완하고 예측 성능을 높이는 기법
    - 주의점
        - 계산 비용이 높아 대규모 데이터셋에서 속도 저하
        - 모델 해석력이 낮아 결과를 설명하기 어려움
    - 모델 종류
        - RandomForest
        - XGBoost
        - LightGBM

### **데이터 스케일링**

- 데이터의 크기, 단위, 분포 차이를 조정하여 머신러닝 모델의 성능을 향상시키기 위한 과정
- 많은 알고리즘은 변수의 크기에 영향을 받기 때문에 값이 큰 데이터가 더 큰 영향을 미치지 않도록 조정 필요
- 단위나 범위가 크게 다를 경우, 특정 특징이 다른 특징을 압도하는 문제를 방지

#### **종류**

1. **Standard Scaler**
    - 표준화 방식
    - 데이터의 평균을 0, 표준편차를 1로 조정
    - $z = \frac{x - \mu}{\sigma}$
2. **Min-Max Scaler**
    - 정규화 방식
    - 데이터 값을 0과 1 사이의 값으로 스케일링
    - $x' = \frac{x - x_{min}}{x_{max} - x_{min}}$
3. **Log 변환**
    - 데이터의 분포가 심하게 왜곡된 경우 로그 적용
    - 음수나 0 값은 변환 불가
    - $x' = \log(x + 1)$

---

## **자연어 데이터**

### **문제점**

- **비정형 데이터**: 틀이 잡혀 있지 않고 사전 정의가 없는 데이터
- **희소성**: 높은 차원에 비해 특정 단어가 드물게 나타나, 문장 간 유사도를 파악하기 어려움
- **모호성과 중의성**: 단어의 다의성과 문맥 의존성
    - (예: "Apple": 과일 vs 회사)
- **다양성**: 동일한 의미를 가진 다양한 표현
    - (예: 좋아요 -> 좋아, 굿, nice)
- **비표준 표현**: 대/소문자, 띄어쓰기, 맞춤법, 오탈자, 문법 오류
- **불필요한 단어**: 길이가 짧은 단어, 등장빈도가 적은 단어, 특수기호
- **비구조화**: 컴퓨터가 자연어 데이터를 인식 및 처리할 수 없음

### **문제 해결 방법**

1. **텍스트 정제(Text Cleaning)**
    - 텍스트 데이터에서 의미가 없는 기호, 숫자, 불용어 등을 제거
        - 불용어 : "the", "is", "것", "들"와 같이 의미가 없는 단어를 뜻함
    - 예시
        - URL, HTML 태그 제거
2. **텍스트 정규화(Text Normalization)**
    - 데이터를 일관된 형식으로 변환하여 모델이 학습하기 쉽도록 만듦
    - 구어체, 약어, 이모티콘 등 다양한 표현을 문법적, 의미적 표준 형태로 통일함
    - 예시
        - 소문자 변환
        - 철자 교정
    
    | **정규표현식** | **설명**                    |
    | -------------- | --------------------------- |
    | [0-9]          | 모든 숫자                   |
    | [a-zA-Z]       | 모든 알파벳                 |
    | \d             | 숫자 매칭, [0-9] 동일표현   |
    | \D             | 숫자가 아닌 것과 매칭       |
    | [^xy]          | ^는 not 표현, 즉 x,y를 제외 |
    
    ```python
    import re
    
    def text_normalization(text):
        # 1. 소문자로 변환
        text = text.lower()
        # 2. HTML 태그 제거
        text = re.sub(r'<.*?>', '', text)
        # 3. URL 제거
        text = re.sub(r'http\\S+|www\\S+', '', text)
        # 4. 숫자 제거
        text = re.sub(r'\\d+', '', text)
        # 5. 특수문자 제거
        text = re.sub(r'[^\\w\\s]', '', text)
        # 6. 불필요한 공백 제거
        text = re.sub(r'\\s+', ' ', text).strip()
        return text
    
    ```
1. **토큰화(Tokenization)**
    - 텍스트를 단어, 문장, 서브워드 등으로 분리하여 **분석이 가능한 형태로 만듦**
    - 텍스트 분석에서 다룰 수 있는 최소 단위로 구조화하여 모델이 이해할 수 있게 함
    - 라이브러리
        - transformers
        - konlpy(한국어 적용)
        
        | **종류**  | **문장**                    | **결과**                         |
        | --------- | --------------------------- | -------------------------------- |
        | 단어 단위 | "I love NLP!"               | ["I", "love", "NLP!"]            |
        | 문장 단위 | "I love NLP. It's amazing!" | ["I love NLP.", "It's amazing!"] |
        | 서브워드  | "unhappiness"               | ["un", "happi", "ness"]          |

2. **텍스트 벡터화(Text Vectorization)**
    - 자연어 데이터를 수치화하여 컴퓨터가 처리할 수 있는 형태로 변환

    | **벡터화 방식**       | **특징**                                        | **장점**                 | **단점**         |
    | --------------------- | ----------------------------------------------- | ------------------------ | ---------------- |
    | Bag-of-Words          | 단어의 단순 빈도를 기반                         | 간단하고 구현 용이       | 문맥 정보 부족   |
    | 고차원 희소 행렬 생성 |
    | TF-IDF                | 단어 빈도와 역문서 빈도로 가중치를 부여         | 단어의 중요도 반영       |
    | 불필요한 단어 제거    | 문맥 정보 부족                                  |
    | Word2Vec/GloVe        | 단어를 밀집 벡터로 표현하여 단어 간 관계를 포함 | 단어 간 유사성 표현 가능 | 문맥 정보 부족   |
    | 문장 수준 표현 어려움 |
    | BERT/GPT              | 문맥적 의미를 반영한 벡터화                     | 문맥과 순서를 고려       | 계산 비용이 높음 |
    | 고사양 하드웨어 필요  |


    1. **TF-IDF(Term Frequency-Inverse Document Frequency)**
      - 텍스트 데이터에서 특정 단어의 중요도를 평가하기 위한 통계적 방법
      - 단어의 빈도(TF)와 역문서 빈도(IDF)를 조합하여 가중치를 부여
      - 수식 : $TF-IDF(t, d, D) = \frac{f(t, d)}{ N_d} \times \log \frac{N}{1+n(t)}$
        - $f(t, d)$ : 단어 t가 문서 d에서 등장한 횟수
        - $N_d$ : 문서 d의 전체 단어 수
        - $N$ : 전체 문서의 수
        - $n(t)$ : 단어 t가 등장한 문서의 수
    2. **Word2Vec**
        - 단어를 고차원 벡터로 매핑하는 임베딩 방법으로, 단어의 **문맥적 관계를 학습함**
        - 단어 간 수학적 연산 가능
        - 의미적 유사성 반영
            - 예시 : king - man + woman = queen
        - CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 모델 구조를 사용하여 효율적 학습 가능
            
            ![image.png](assets/img/posts/study/AI/데이터 품질/image%202.png)

            - CBOW(Continuous Bag of Words)
                - 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하며, 중심 단어가 회귀 단어일 때 유리함
                
                ![https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)
                
                ![https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)
                
            - Skip-Gram
                - 중심 단어에서 주변 단어를 예측하며, 주변 단어가 희귀 단어일 때 유리함
                
                ![https://wikidocs.net/images/page/22660/skipgram_dataset.PNG](https://wikidocs.net/images/page/22660/skipgram_dataset.PNG)
                
                ![https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)

---

## Reference

1. [https://blog-ko.superb-ai.com/data-is-the-key-to-artificial-intelligence/](https://blog-ko.superb-ai.com/data-is-the-key-to-artificial-intelligence/)
2. [https://www.donga.com/news/It/article/all/20220824/115122895/1](https://www.donga.com/news/It/article/all/20220824/115122895/1)
3. [https://cphinf.pstatic.net/playsw/20220117_87/1642385853602Ytnye_JPEG/ETH_2_01_00_A_01_v1.jpg?type=w700](https://cphinf.pstatic.net/playsw/20220117_87/1642385853602Ytnye_JPEG/ETH_2_01_00_A_01_v1.jpg?type=w700)
4. [https://www.tta.or.kr/data/androReport/ttaJnal/182-1-3-6.pdf](https://www.tta.or.kr/data/androReport/ttaJnal/182-1-3-6.pdf)
5. [https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00276/43518/Natural-Questions-A-Benchmark-for-Question?source=post_page](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00276/43518/Natural-Questions-A-Benchmark-for-Question?source=post_page)
6. [https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/)
7. [https://yozm.wishket.com/magazine/detail/1919/](https://yozm.wishket.com/magazine/detail/1919/)
8. [https://slideplayer.com/slide/6394283/](https://slideplayer.com/slide/6394283/)
9. [https://www.kdnuggets.com/2019/11/understanding-boxplots.html](https://www.kdnuggets.com/2019/11/understanding-boxplots.html)
10. [https://spotintelligence.com/2024/05/21/isolation-forest/](https://spotintelligence.com/2024/05/21/isolation-forest/)
11. [https://www.researchgate.net/figure/Density-based-spatial-clustering-of-applications-with-noise-DBSCAN-concept_fig3_331324525](https://www.researchgate.net/figure/Density-based-spatial-clustering-of-applications-with-noise-DBSCAN-concept_fig3_331324525)
12. [https://medium.com/@fraidoonomarzai99/word2vec-cbow-skip-gram-in-depth-88d9cc340a50](https://medium.com/@fraidoonomarzai99/word2vec-cbow-skip-gram-in-depth-88d9cc340a50)
13. [https://wikidocs.net/22660](https://wikidocs.net/22660)
