---
layout: post
title: "[모두를 위한 딥러닝 시즌2] Lab-11-0 RNN intro"
date: 2024-12-04 21:01:00+0900
categories: [Study, AI]
tags: [Deep Learning Zero To All, 모두를 위한 딥러닝 시즌2, ML, DL, pytorch]
math: true
mermaid : true
---
## RNN이란?

![RNN의 구조](assets/img/posts/AI/11-0/image.png)

RNN의 구조

- Sequential Data를 처리하기 위한 대표적인 알고리즘
- 데이터의 순서와 문맥(Context)을 학습하는 데 효과적
- Sequential Data: 데이터 자체뿐만 아니라 순서가 중요한 데이터 유형
    - 예: "Hello"라는 단어는 글자의 순서에 따라 의미가 결정됨

## RNN 구조

### 핵심 개념

1. **Hidden State**
    - 정의
        - 이전 단계의 출력 정보를 저장하여, 다음 단계의 입력 데이터와 함께 연산에 활용하는 메모리 역할
        - Time Step(시간 단계)가 진행될수록 정보를 전달하며 데이터 간 관계를 학습
    - 특징
        - 시퀀스 간의 문맥(Context)과 의존성을 학습하는 데 중요한 역할
2. **Position Index** 
    - 정의
        - 데이터의 순서를 나타내는 정보로, 순서에 따른 문맥을 이해하도록 모델에 추가되는 정보
        - 시퀀셜 데이터에서 입력 벡터의 순서를 학습할 수 있도록 도움
    - 예시
        - 텍스트 데이터에서 "Hello"와 "oHell"은 같은 글자지만,**Position Index**를 통해 의미가 달라짐을 학습
3. **Parameter Sharing (파라미터 공유)**
    - 정의
        - RNN은 모든 시간 단계에서 동일한 파라미터(Weights)를 사용하여 학습
        - 이를 통해 모델의 크기를 축소하고, 학습 효율성을 향상
    - 특징
        - 데이터가 긴 시퀀스일수록 메모리 사용량 절감
        - 동일한 파라미터를 공유하므로 모델의 일관성을 유지

### 작동 원리

- 입력되는 **vector(data)**에 **matrix(Weight)**를 곱하여 출력 값 계산
- Time Step마다 반복적으로 진행
    
![image.png](assets/img/posts/AI/11-0/image%201.png)
    
1. 데이터 입력
    - 입력되는 vector(data)는 RNN의 입력으로 전달
2. Position Index 추가
    - 입력 벡터 앞에 데이터 순서를 나타내는 **position index**를 추가하여 문맥과 순서를 이해
3. Weight 연산
    - 입력 데이터와 히든 스테이트에 각각 가중치(Matrix)를 곱한 후 더한 결과를 활성화 함수에 전달
4. 활성화 함수를 통해 출력 값을 생성

### 수식

![image.png](assets/img/posts/AI/11-0/image%202.png)

$$
h_t = f(h_{t-1},x_t)
$$

$$
h_t = \tanh(W_h h_{t-1} + W_x x_t)
$$

- $h_{t}$: 현재 시간 단계의 출력 (Hidden State)
- $h_{t-1}$: 이전 단계의 Hidden State
- $x_t$: 현재 입력 데이터
- $W_h$: 히든 스테이트 가중치 행렬
- $W_x$: 입력 데이터 가중치 행렬

가중치의 역할

1. $W_h$: 이전 Hidden State $h_{t−1}$가 현재 출력에 얼마나 영향을 줄지 결정
2. $W_x$: 현재 입력 데이터 $x_t$가 출력에 얼마나 영향을 줄지 결정

## 작동 예시

![RNN 작동 구조](assets/img/posts/AI/11-0/image%203.png)

1. 첫 단계 (t=0)
    - 초기 히든 스테이트 $h_0$는 일반적으로 0으로 초기화
    - 입력 $x_0$와 $W_x$를 곱하고, 활성화 함수로 처리하여 $h_1$ 생성
2. 이후 단계 (t=n)
    - $h_{n-1}$와 $x_n$를 결합하여  $h_n$계산
    - 이전 출력 정보가 다음 계산에 반영됨
    

## 입출력 구조

![RNN의 입출력 구조](assets/img/posts/AI/11-0/image%204.png)

- One-to-One (neural network)
    - 예: 단일 이미지 → 하나의 분류 결과
- One-to-Many
    - 예: 이미지 → 캡션 생성
- Many-to-One
    - 예: 문장 → 감정 레이블
    - 출력값이 여러 개가 나오긴 하지만 해당되는 출력값 외에는 무시된다
- Many-to-Many
    - 예: 번역 (영어 문장 → 한국어 문장)

## LSTM과 GRU

- 대표적인 RNN 모델

### LSTM (Long Short-Term Memory)

- 특징
    - 셀 상태(cell state)와 게이트(gate) 구조를 활용해 정보 흐름 조절
    - 장기 의존성(Long-Term Dependency) 문제 해결
- 사용 예시
    - 장문의 텍스트 분석
    - 주식 시장 예측 등 시계열 데이터

### GRU (Gated Recurrent Unit)

- 특징
    - LSTM보다 간단한 구조로 학습 자원 절약
    - 업데이트 게이트와 리셋 게이트 사용
- 사용 예시
    - 실시간 데이터 처리
    - 음성 인식
